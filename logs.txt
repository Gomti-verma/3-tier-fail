
==> Audit <==
|------------|-----------------|----------|-------------|---------|---------------------|---------------------|
|  Command   |      Args       | Profile  |    User     | Version |     Start Time      |      End Time       |
|------------|-----------------|----------|-------------|---------|---------------------|---------------------|
| start      | --driver=docker | minikube | gomti-verma | v1.35.0 | 09 May 25 12:29 IST |                     |
| docker-env |                 | minikube | gomti-verma | v1.35.0 | 09 May 25 13:03 IST |                     |
| docker-env |                 | minikube | gomti-verma | v1.35.0 | 09 May 25 13:10 IST |                     |
| start      |                 | minikube | gomti-verma | v1.35.0 | 09 May 25 13:27 IST |                     |
| start      | --driver=docker | minikube | gomti-verma | v1.35.0 | 09 May 25 13:28 IST | 09 May 25 13:35 IST |
| docker-env |                 | minikube | gomti-verma | v1.35.0 | 09 May 25 13:38 IST | 09 May 25 13:38 IST |
| service    | frontend        | minikube | gomti-verma | v1.35.0 | 09 May 25 13:41 IST |                     |
| service    | frontend        | minikube | gomti-verma | v1.35.0 | 09 May 25 13:43 IST |                     |
|------------|-----------------|----------|-------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/05/09 13:28:28
Running on machine: gomti-verma-Latitude-5480
Binary: Built with gc go1.23.4 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0509 13:28:28.337888   59268 out.go:345] Setting OutFile to fd 1 ...
I0509 13:28:28.340246   59268 out.go:397] isatty.IsTerminal(1) = true
I0509 13:28:28.340259   59268 out.go:358] Setting ErrFile to fd 2...
I0509 13:28:28.340277   59268 out.go:397] isatty.IsTerminal(2) = true
I0509 13:28:28.341044   59268 root.go:338] Updating PATH: /home/gomti-verma/.minikube/bin
W0509 13:28:28.341447   59268 root.go:314] Error reading config file at /home/gomti-verma/.minikube/config/config.json: open /home/gomti-verma/.minikube/config/config.json: no such file or directory
I0509 13:28:28.344456   59268 out.go:352] Setting JSON to false
I0509 13:28:28.348798   59268 start.go:129] hostinfo: {"hostname":"gomti-verma-Latitude-5480","uptime":5908,"bootTime":1746771600,"procs":268,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"24.04","kernelVersion":"6.11.0-24-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"10166eac-372d-4dee-b255-5a50b9230298"}
I0509 13:28:28.349071   59268 start.go:139] virtualization: kvm host
I0509 13:28:28.358872   59268 out.go:177] 😄  minikube v1.35.0 on Ubuntu 24.04
I0509 13:28:28.366040   59268 notify.go:220] Checking for updates...
I0509 13:28:28.367981   59268 driver.go:394] Setting default libvirt URI to qemu:///system
I0509 13:28:28.447852   59268 docker.go:123] docker version: linux-28.1.1:Docker Engine - Community
I0509 13:28:28.448402   59268 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0509 13:28:28.538423   59268 info.go:266] docker info: {ID:d150000f-e443-40fa-ba09-84e82eec9f4d Containers:9 ContainersRunning:0 ContainersPaused:0 ContainersStopped:9 Images:19 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:22 OomKillDisable:false NGoroutines:42 SystemTime:2025-05-09 13:28:28.52201896 +0530 IST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.11.0-24-generic OperatingSystem:Ubuntu 24.04.2 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8201592832 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:gomti-verma-Latitude-5480 Labels:[] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.35.1]] Warnings:<nil>}}
I0509 13:28:28.538533   59268 docker.go:318] overlay module found
I0509 13:28:28.544940   59268 out.go:177] ✨  Using the docker driver based on user configuration
I0509 13:28:28.552129   59268 start.go:297] selected driver: docker
I0509 13:28:28.552137   59268 start.go:901] validating driver "docker" against <nil>
I0509 13:28:28.552150   59268 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0509 13:28:28.552250   59268 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0509 13:28:28.634292   59268 info.go:266] docker info: {ID:d150000f-e443-40fa-ba09-84e82eec9f4d Containers:9 ContainersRunning:0 ContainersPaused:0 ContainersStopped:9 Images:19 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:22 OomKillDisable:false NGoroutines:42 SystemTime:2025-05-09 13:28:28.61997407 +0530 IST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.11.0-24-generic OperatingSystem:Ubuntu 24.04.2 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8201592832 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:gomti-verma-Latitude-5480 Labels:[] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.35.1]] Warnings:<nil>}}
I0509 13:28:28.634525   59268 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0509 13:28:28.635408   59268 start_flags.go:393] Using suggested 2200MB memory alloc based on sys=7821MB, container=7821MB
I0509 13:28:28.635637   59268 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0509 13:28:28.641744   59268 out.go:177] 📌  Using Docker driver with root privileges
I0509 13:28:28.648265   59268 cni.go:84] Creating CNI manager for ""
I0509 13:28:28.648353   59268 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0509 13:28:28.648360   59268 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0509 13:28:28.648463   59268 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/gomti-verma:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0509 13:28:28.660602   59268 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0509 13:28:28.668393   59268 cache.go:121] Beginning downloading kic base image for docker with docker
I0509 13:28:28.675553   59268 out.go:177] 🚜  Pulling base image v0.0.46 ...
I0509 13:28:28.681563   59268 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0509 13:28:28.681693   59268 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0509 13:28:28.711802   59268 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 to local cache
I0509 13:28:28.711973   59268 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory
I0509 13:28:28.711990   59268 image.go:68] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory, skipping pull
I0509 13:28:28.711994   59268 image.go:137] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in cache, skipping pull
I0509 13:28:28.712018   59268 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 as a tarball
I0509 13:28:28.712022   59268 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from local cache
I0509 13:28:28.712176   59268 cache.go:169] failed to download gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279, will try fallback image if available: tarball: unexpected EOF
I0509 13:28:28.712183   59268 image.go:81] Checking for docker.io/kicbase/stable:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0509 13:28:28.737303   59268 cache.go:150] Downloading docker.io/kicbase/stable:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 to local cache
I0509 13:28:28.737586   59268 image.go:65] Checking for docker.io/kicbase/stable:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory
I0509 13:28:28.737642   59268 image.go:150] Writing docker.io/kicbase/stable:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 to local cache
I0509 13:28:54.166380   59268 cache.go:169] failed to download docker.io/kicbase/stable:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279, will try fallback image if available: getting remote image: Get "https://index.docker.io/v2/": dial tcp: lookup index.docker.io on 127.0.0.53:53: server misbehaving
I0509 13:28:54.166412   59268 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46 in local docker daemon
I0509 13:28:54.229128   59268 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.46 to local cache
I0509 13:28:54.229454   59268 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46 in local cache directory
I0509 13:28:54.229520   59268 image.go:150] Writing gcr.io/k8s-minikube/kicbase:v0.0.46 to local cache
I0509 13:28:58.305683   59268 preload.go:118] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.32.0/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0509 13:28:58.305750   59268 cache.go:56] Caching tarball of preloaded images
I0509 13:28:58.306014   59268 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0509 13:28:58.312893   59268 out.go:177] 💾  Downloading Kubernetes v1.32.0 preload ...
I0509 13:28:58.319470   59268 preload.go:236] getting checksum for preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 ...
I0509 13:28:58.776473   59268 download.go:108] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.32.0/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4?checksum=md5:4da2ed9bc13e09e8e9b7cf53d01335db -> /home/gomti-verma/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0509 13:34:08.819065   59268 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.46 as a tarball
I0509 13:34:08.819083   59268 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.46 from local cache
I0509 13:34:32.211369   59268 preload.go:247] saving checksum for preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 ...
I0509 13:34:32.211434   59268 preload.go:254] verifying checksum of /home/gomti-verma/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 ...
I0509 13:34:32.989106   59268 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0509 13:34:32.989411   59268 profile.go:143] Saving config to /home/gomti-verma/.minikube/profiles/minikube/config.json ...
I0509 13:34:32.989432   59268 lock.go:35] WriteFile acquiring /home/gomti-verma/.minikube/profiles/minikube/config.json: {Name:mk112efc0ec0c4dc2d55fab5b317dcba8bd38824 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0509 13:34:33.323981   59268 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.46 from cached tarball
I0509 13:34:33.324027   59268 cache.go:227] Successfully downloaded all kic artifacts
I0509 13:34:33.324075   59268 start.go:360] acquireMachinesLock for minikube: {Name:mk485a6f56c7d465f26d9349c5ce2908aa8ae32b Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0509 13:34:33.324299   59268 start.go:364] duration metric: took 194.114µs to acquireMachinesLock for "minikube"
I0509 13:34:33.324356   59268 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/gomti-verma:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0509 13:34:33.324495   59268 start.go:125] createHost starting for "" (driver="docker")
I0509 13:34:33.337528   59268 out.go:235] 🔥  Creating docker container (CPUs=2, Memory=2200MB) ...
I0509 13:34:33.338152   59268 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0509 13:34:33.338190   59268 client.go:168] LocalClient.Create starting
I0509 13:34:33.338419   59268 main.go:141] libmachine: Creating CA: /home/gomti-verma/.minikube/certs/ca.pem
I0509 13:34:33.815010   59268 main.go:141] libmachine: Creating client certificate: /home/gomti-verma/.minikube/certs/cert.pem
I0509 13:34:34.103877   59268 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0509 13:34:34.121195   59268 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0509 13:34:34.121248   59268 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0509 13:34:34.121279   59268 cli_runner.go:164] Run: docker network inspect minikube
W0509 13:34:34.137234   59268 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0509 13:34:34.137271   59268 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0509 13:34:34.137280   59268 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0509 13:34:34.137352   59268 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0509 13:34:34.155457   59268 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc001978a40}
I0509 13:34:34.155479   59268 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0509 13:34:34.155516   59268 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0509 13:34:34.239831   59268 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0509 13:34:34.239848   59268 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0509 13:34:34.239909   59268 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0509 13:34:34.267090   59268 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0509 13:34:34.292258   59268 oci.go:103] Successfully created a docker volume minikube
I0509 13:34:34.292319   59268 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.46 -d /var/lib
I0509 13:34:35.582387   59268 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.46 -d /var/lib: (1.290043348s)
I0509 13:34:35.582406   59268 oci.go:107] Successfully prepared a docker volume minikube
I0509 13:34:35.582418   59268 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0509 13:34:35.582432   59268 kic.go:194] Starting extracting preloaded images to volume ...
I0509 13:34:35.582483   59268 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/gomti-verma/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46 -I lz4 -xf /preloaded.tar -C /extractDir
I0509 13:34:38.891826   59268 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/gomti-verma/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46 -I lz4 -xf /preloaded.tar -C /extractDir: (3.309308282s)
I0509 13:34:38.891845   59268 kic.go:203] duration metric: took 3.309408735s to extract preloaded images to volume ...
W0509 13:34:38.891965   59268 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W0509 13:34:38.892010   59268 oci.go:249] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I0509 13:34:38.892054   59268 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0509 13:34:39.262209   59268 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.46
I0509 13:34:39.926692   59268 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0509 13:34:39.950125   59268 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0509 13:34:39.973803   59268 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0509 13:34:40.033748   59268 oci.go:144] the created container "minikube" has a running status.
I0509 13:34:40.033762   59268 kic.go:225] Creating ssh key for kic: /home/gomti-verma/.minikube/machines/minikube/id_rsa...
I0509 13:34:40.201872   59268 kic_runner.go:191] docker (temp): /home/gomti-verma/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0509 13:34:40.331352   59268 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0509 13:34:40.350664   59268 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0509 13:34:40.350672   59268 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0509 13:34:40.399959   59268 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0509 13:34:40.416926   59268 machine.go:93] provisionDockerMachine start ...
I0509 13:34:40.417007   59268 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0509 13:34:40.549972   59268 main.go:141] libmachine: Using SSH client type: native
I0509 13:34:40.550148   59268 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0509 13:34:40.550154   59268 main.go:141] libmachine: About to run SSH command:
hostname
I0509 13:34:40.551566   59268 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:32962->127.0.0.1:32768: read: connection reset by peer
I0509 13:34:43.552483   59268 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:32976->127.0.0.1:32768: read: connection reset by peer
I0509 13:34:46.714378   59268 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0509 13:34:46.714391   59268 ubuntu.go:169] provisioning hostname "minikube"
I0509 13:34:46.714443   59268 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0509 13:34:46.751818   59268 main.go:141] libmachine: Using SSH client type: native
I0509 13:34:46.752057   59268 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0509 13:34:46.752067   59268 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0509 13:34:46.940980   59268 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0509 13:34:46.941096   59268 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0509 13:34:46.970901   59268 main.go:141] libmachine: Using SSH client type: native
I0509 13:34:46.971091   59268 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0509 13:34:46.971105   59268 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0509 13:34:47.135534   59268 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0509 13:34:47.135550   59268 ubuntu.go:175] set auth options {CertDir:/home/gomti-verma/.minikube CaCertPath:/home/gomti-verma/.minikube/certs/ca.pem CaPrivateKeyPath:/home/gomti-verma/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/gomti-verma/.minikube/machines/server.pem ServerKeyPath:/home/gomti-verma/.minikube/machines/server-key.pem ClientKeyPath:/home/gomti-verma/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/gomti-verma/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/gomti-verma/.minikube}
I0509 13:34:47.135576   59268 ubuntu.go:177] setting up certificates
I0509 13:34:47.135589   59268 provision.go:84] configureAuth start
I0509 13:34:47.135644   59268 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0509 13:34:47.159189   59268 provision.go:143] copyHostCerts
I0509 13:34:47.159254   59268 exec_runner.go:151] cp: /home/gomti-verma/.minikube/certs/ca.pem --> /home/gomti-verma/.minikube/ca.pem (1090 bytes)
I0509 13:34:47.159723   59268 exec_runner.go:151] cp: /home/gomti-verma/.minikube/certs/cert.pem --> /home/gomti-verma/.minikube/cert.pem (1135 bytes)
I0509 13:34:47.159815   59268 exec_runner.go:151] cp: /home/gomti-verma/.minikube/certs/key.pem --> /home/gomti-verma/.minikube/key.pem (1679 bytes)
I0509 13:34:47.159887   59268 provision.go:117] generating server cert: /home/gomti-verma/.minikube/machines/server.pem ca-key=/home/gomti-verma/.minikube/certs/ca.pem private-key=/home/gomti-verma/.minikube/certs/ca-key.pem org=gomti-verma.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0509 13:34:47.347603   59268 provision.go:177] copyRemoteCerts
I0509 13:34:47.347644   59268 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0509 13:34:47.347676   59268 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0509 13:34:47.374893   59268 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/gomti-verma/.minikube/machines/minikube/id_rsa Username:docker}
I0509 13:34:47.476913   59268 ssh_runner.go:362] scp /home/gomti-verma/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1090 bytes)
I0509 13:34:47.556774   59268 ssh_runner.go:362] scp /home/gomti-verma/.minikube/machines/server.pem --> /etc/docker/server.pem (1192 bytes)
I0509 13:34:47.603762   59268 ssh_runner.go:362] scp /home/gomti-verma/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0509 13:34:47.659477   59268 provision.go:87] duration metric: took 523.87678ms to configureAuth
I0509 13:34:47.659492   59268 ubuntu.go:193] setting minikube options for container-runtime
I0509 13:34:47.659685   59268 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0509 13:34:47.659740   59268 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0509 13:34:47.690139   59268 main.go:141] libmachine: Using SSH client type: native
I0509 13:34:47.690382   59268 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0509 13:34:47.690393   59268 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0509 13:34:47.865404   59268 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0509 13:34:47.865433   59268 ubuntu.go:71] root file system type: overlay
I0509 13:34:47.865586   59268 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0509 13:34:47.865690   59268 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0509 13:34:47.890378   59268 main.go:141] libmachine: Using SSH client type: native
I0509 13:34:47.890520   59268 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0509 13:34:47.890582   59268 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0509 13:34:48.068346   59268 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0509 13:34:48.068433   59268 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0509 13:34:48.098427   59268 main.go:141] libmachine: Using SSH client type: native
I0509 13:34:48.098724   59268 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0509 13:34:48.098745   59268 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0509 13:34:50.005086   59268 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-12-17 15:44:19.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-05-09 08:04:48.065004415 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0509 13:34:50.005103   59268 machine.go:96] duration metric: took 9.588166798s to provisionDockerMachine
I0509 13:34:50.005112   59268 client.go:171] duration metric: took 16.666916303s to LocalClient.Create
I0509 13:34:50.005125   59268 start.go:167] duration metric: took 16.666980585s to libmachine.API.Create "minikube"
I0509 13:34:50.005131   59268 start.go:293] postStartSetup for "minikube" (driver="docker")
I0509 13:34:50.005142   59268 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0509 13:34:50.005202   59268 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0509 13:34:50.005245   59268 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0509 13:34:50.043410   59268 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/gomti-verma/.minikube/machines/minikube/id_rsa Username:docker}
I0509 13:34:50.177375   59268 ssh_runner.go:195] Run: cat /etc/os-release
I0509 13:34:50.183128   59268 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0509 13:34:50.183155   59268 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0509 13:34:50.183165   59268 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0509 13:34:50.183171   59268 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0509 13:34:50.183181   59268 filesync.go:126] Scanning /home/gomti-verma/.minikube/addons for local assets ...
I0509 13:34:50.183269   59268 filesync.go:126] Scanning /home/gomti-verma/.minikube/files for local assets ...
I0509 13:34:50.183654   59268 start.go:296] duration metric: took 178.516411ms for postStartSetup
I0509 13:34:50.184039   59268 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0509 13:34:50.210996   59268 profile.go:143] Saving config to /home/gomti-verma/.minikube/profiles/minikube/config.json ...
I0509 13:34:50.212582   59268 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0509 13:34:50.212625   59268 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0509 13:34:50.244847   59268 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/gomti-verma/.minikube/machines/minikube/id_rsa Username:docker}
I0509 13:34:50.370072   59268 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0509 13:34:50.380928   59268 start.go:128] duration metric: took 17.056401713s to createHost
I0509 13:34:50.380942   59268 start.go:83] releasing machines lock for "minikube", held for 17.056630967s
I0509 13:34:50.381017   59268 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0509 13:34:50.418365   59268 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0509 13:34:50.418446   59268 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0509 13:34:50.418637   59268 ssh_runner.go:195] Run: cat /version.json
I0509 13:34:50.418676   59268 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0509 13:34:50.454359   59268 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/gomti-verma/.minikube/machines/minikube/id_rsa Username:docker}
I0509 13:34:50.455591   59268 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/gomti-verma/.minikube/machines/minikube/id_rsa Username:docker}
I0509 13:34:50.562137   59268 ssh_runner.go:195] Run: systemctl --version
I0509 13:34:55.835731   59268 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (5.417326484s)
W0509 13:34:55.835775   59268 start.go:867] [curl -sS -m 2 https://registry.k8s.io/] failed: curl -sS -m 2 https://registry.k8s.io/: Process exited with status 28
stdout:

stderr:
curl: (28) Resolving timed out after 2000 milliseconds
I0509 13:34:55.835812   59268 ssh_runner.go:235] Completed: systemctl --version: (5.273632138s)
I0509 13:34:55.836034   59268 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0509 13:34:55.852091   59268 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0509 13:34:55.975852   59268 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0509 13:34:55.976336   59268 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0509 13:34:56.053552   59268 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0509 13:34:56.053573   59268 start.go:495] detecting cgroup driver to use...
I0509 13:34:56.053614   59268 detect.go:190] detected "systemd" cgroup driver on host os
I0509 13:34:56.056019   59268 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0509 13:34:56.085692   59268 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0509 13:34:56.103942   59268 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0509 13:34:56.119340   59268 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0509 13:34:56.119384   59268 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0509 13:34:56.132232   59268 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0509 13:34:56.146017   59268 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0509 13:34:56.159053   59268 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0509 13:34:56.175282   59268 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0509 13:34:56.186959   59268 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0509 13:34:56.199999   59268 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0509 13:34:56.212466   59268 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0509 13:34:56.225503   59268 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0509 13:34:56.237774   59268 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0509 13:34:56.250509   59268 ssh_runner.go:195] Run: sudo systemctl daemon-reload
W0509 13:34:56.305962   59268 out.go:270] ❗  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0509 13:34:56.306207   59268 out.go:270] 💡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0509 13:34:56.354997   59268 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0509 13:34:56.462585   59268 start.go:495] detecting cgroup driver to use...
I0509 13:34:56.462611   59268 detect.go:190] detected "systemd" cgroup driver on host os
I0509 13:34:56.462650   59268 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0509 13:34:56.482536   59268 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0509 13:34:56.482666   59268 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0509 13:34:56.559734   59268 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0509 13:34:56.621356   59268 ssh_runner.go:195] Run: which cri-dockerd
I0509 13:34:56.633375   59268 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0509 13:34:56.684870   59268 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0509 13:34:56.750191   59268 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0509 13:34:57.010714   59268 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0509 13:34:57.292642   59268 docker.go:574] configuring docker to use "systemd" as cgroup driver...
I0509 13:34:57.292808   59268 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0509 13:34:57.324713   59268 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0509 13:34:57.423548   59268 ssh_runner.go:195] Run: sudo systemctl restart docker
I0509 13:34:59.579935   59268 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.156327505s)
I0509 13:34:59.580084   59268 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0509 13:34:59.681241   59268 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0509 13:34:59.722214   59268 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0509 13:34:59.984773   59268 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0509 13:35:00.252796   59268 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0509 13:35:00.500117   59268 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0509 13:35:00.555958   59268 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0509 13:35:00.594649   59268 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0509 13:35:00.846235   59268 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0509 13:35:01.308343   59268 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0509 13:35:01.308456   59268 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0509 13:35:01.320432   59268 start.go:563] Will wait 60s for crictl version
I0509 13:35:01.320546   59268 ssh_runner.go:195] Run: which crictl
I0509 13:35:01.331736   59268 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0509 13:35:01.620639   59268 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0509 13:35:01.620751   59268 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0509 13:35:01.827305   59268 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0509 13:35:01.909027   59268 out.go:235] 🐳  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0509 13:35:01.909214   59268 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0509 13:35:01.962866   59268 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0509 13:35:01.976192   59268 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0509 13:35:02.080831   59268 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/gomti-verma:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0509 13:35:02.081088   59268 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0509 13:35:02.081238   59268 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0509 13:35:02.176254   59268 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0509 13:35:02.176273   59268 docker.go:619] Images already preloaded, skipping extraction
I0509 13:35:02.176429   59268 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0509 13:35:02.231456   59268 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0509 13:35:02.231473   59268 cache_images.go:84] Images are preloaded, skipping loading
I0509 13:35:02.231482   59268 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0509 13:35:02.231569   59268 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0509 13:35:02.231633   59268 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0509 13:35:02.590588   59268 cni.go:84] Creating CNI manager for ""
I0509 13:35:02.590602   59268 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0509 13:35:02.590680   59268 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0509 13:35:02.590697   59268 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0509 13:35:02.590807   59268 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0509 13:35:02.590856   59268 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0509 13:35:02.605215   59268 binaries.go:44] Found k8s binaries, skipping transfer
I0509 13:35:02.605262   59268 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0509 13:35:02.616323   59268 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0509 13:35:02.641211   59268 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0509 13:35:02.665383   59268 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2285 bytes)
I0509 13:35:02.690033   59268 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0509 13:35:02.694719   59268 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0509 13:35:02.709235   59268 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0509 13:35:02.809242   59268 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0509 13:35:02.835227   59268 certs.go:68] Setting up /home/gomti-verma/.minikube/profiles/minikube for IP: 192.168.49.2
I0509 13:35:02.835236   59268 certs.go:194] generating shared ca certs ...
I0509 13:35:02.835249   59268 certs.go:226] acquiring lock for ca certs: {Name:mk356922b28a484e286bc35a652eec74111ab53a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0509 13:35:02.835362   59268 certs.go:240] generating "minikubeCA" ca cert: /home/gomti-verma/.minikube/ca.key
I0509 13:35:03.329139   59268 crypto.go:156] Writing cert to /home/gomti-verma/.minikube/ca.crt ...
I0509 13:35:03.329150   59268 lock.go:35] WriteFile acquiring /home/gomti-verma/.minikube/ca.crt: {Name:mk2e6eda09bba237fd846fef4cf1d39914632002 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0509 13:35:03.329295   59268 crypto.go:164] Writing key to /home/gomti-verma/.minikube/ca.key ...
I0509 13:35:03.329300   59268 lock.go:35] WriteFile acquiring /home/gomti-verma/.minikube/ca.key: {Name:mkcf631e976959f3b4085a1f3c8af5752c45c701 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0509 13:35:03.329382   59268 certs.go:240] generating "proxyClientCA" ca cert: /home/gomti-verma/.minikube/proxy-client-ca.key
I0509 13:35:03.485650   59268 crypto.go:156] Writing cert to /home/gomti-verma/.minikube/proxy-client-ca.crt ...
I0509 13:35:03.485661   59268 lock.go:35] WriteFile acquiring /home/gomti-verma/.minikube/proxy-client-ca.crt: {Name:mk135ec5d440e62f5cd70dc9598d37e6e083645e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0509 13:35:03.485828   59268 crypto.go:164] Writing key to /home/gomti-verma/.minikube/proxy-client-ca.key ...
I0509 13:35:03.485833   59268 lock.go:35] WriteFile acquiring /home/gomti-verma/.minikube/proxy-client-ca.key: {Name:mk7591e0306fe38150aabaa62035dad2f37fc377 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0509 13:35:03.520752   59268 certs.go:256] generating profile certs ...
I0509 13:35:03.520815   59268 certs.go:363] generating signed profile cert for "minikube-user": /home/gomti-verma/.minikube/profiles/minikube/client.key
I0509 13:35:03.520840   59268 crypto.go:68] Generating cert /home/gomti-verma/.minikube/profiles/minikube/client.crt with IP's: []
I0509 13:35:03.890980   59268 crypto.go:156] Writing cert to /home/gomti-verma/.minikube/profiles/minikube/client.crt ...
I0509 13:35:03.891008   59268 lock.go:35] WriteFile acquiring /home/gomti-verma/.minikube/profiles/minikube/client.crt: {Name:mk0da4d92852172a80327e9d5ae337385b656aa8 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0509 13:35:03.891258   59268 crypto.go:164] Writing key to /home/gomti-verma/.minikube/profiles/minikube/client.key ...
I0509 13:35:03.891264   59268 lock.go:35] WriteFile acquiring /home/gomti-verma/.minikube/profiles/minikube/client.key: {Name:mk8f21d0aa265302ff012c377e6123a5751d3813 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0509 13:35:03.905372   59268 certs.go:363] generating signed profile cert for "minikube": /home/gomti-verma/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0509 13:35:03.905401   59268 crypto.go:68] Generating cert /home/gomti-verma/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0509 13:35:04.049146   59268 crypto.go:156] Writing cert to /home/gomti-verma/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I0509 13:35:04.049158   59268 lock.go:35] WriteFile acquiring /home/gomti-verma/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mk791ecdefd2e8f41e0a8f098d001ebfade98dc9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0509 13:35:04.049319   59268 crypto.go:164] Writing key to /home/gomti-verma/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I0509 13:35:04.049325   59268 lock.go:35] WriteFile acquiring /home/gomti-verma/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mk4603666da2e59d9ce722634b57c76930726d4e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0509 13:35:04.049417   59268 certs.go:381] copying /home/gomti-verma/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /home/gomti-verma/.minikube/profiles/minikube/apiserver.crt
I0509 13:35:04.049509   59268 certs.go:385] copying /home/gomti-verma/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /home/gomti-verma/.minikube/profiles/minikube/apiserver.key
I0509 13:35:04.049576   59268 certs.go:363] generating signed profile cert for "aggregator": /home/gomti-verma/.minikube/profiles/minikube/proxy-client.key
I0509 13:35:04.049591   59268 crypto.go:68] Generating cert /home/gomti-verma/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0509 13:35:04.496406   59268 crypto.go:156] Writing cert to /home/gomti-verma/.minikube/profiles/minikube/proxy-client.crt ...
I0509 13:35:04.496418   59268 lock.go:35] WriteFile acquiring /home/gomti-verma/.minikube/profiles/minikube/proxy-client.crt: {Name:mk1a744f695906b7d3033b819964f4880c5704d4 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0509 13:35:04.496631   59268 crypto.go:164] Writing key to /home/gomti-verma/.minikube/profiles/minikube/proxy-client.key ...
I0509 13:35:04.496637   59268 lock.go:35] WriteFile acquiring /home/gomti-verma/.minikube/profiles/minikube/proxy-client.key: {Name:mk0498a8ac3b3167c6bdc2e2abd5ee1eedd29f3c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0509 13:35:04.496973   59268 certs.go:484] found cert: /home/gomti-verma/.minikube/certs/ca-key.pem (1679 bytes)
I0509 13:35:04.497006   59268 certs.go:484] found cert: /home/gomti-verma/.minikube/certs/ca.pem (1090 bytes)
I0509 13:35:04.497032   59268 certs.go:484] found cert: /home/gomti-verma/.minikube/certs/cert.pem (1135 bytes)
I0509 13:35:04.497056   59268 certs.go:484] found cert: /home/gomti-verma/.minikube/certs/key.pem (1679 bytes)
I0509 13:35:04.498751   59268 ssh_runner.go:362] scp /home/gomti-verma/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0509 13:35:04.529021   59268 ssh_runner.go:362] scp /home/gomti-verma/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0509 13:35:04.563154   59268 ssh_runner.go:362] scp /home/gomti-verma/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0509 13:35:04.593555   59268 ssh_runner.go:362] scp /home/gomti-verma/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0509 13:35:04.624221   59268 ssh_runner.go:362] scp /home/gomti-verma/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0509 13:35:04.655608   59268 ssh_runner.go:362] scp /home/gomti-verma/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0509 13:35:04.694472   59268 ssh_runner.go:362] scp /home/gomti-verma/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0509 13:35:04.727006   59268 ssh_runner.go:362] scp /home/gomti-verma/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0509 13:35:04.761137   59268 ssh_runner.go:362] scp /home/gomti-verma/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0509 13:35:04.809101   59268 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0509 13:35:04.833022   59268 ssh_runner.go:195] Run: openssl version
I0509 13:35:04.845977   59268 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0509 13:35:04.877721   59268 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0509 13:35:04.884147   59268 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 May  9 08:05 /usr/share/ca-certificates/minikubeCA.pem
I0509 13:35:04.884195   59268 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0509 13:35:04.894624   59268 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0509 13:35:04.911795   59268 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0509 13:35:04.916531   59268 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0509 13:35:04.916576   59268 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/gomti-verma:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0509 13:35:04.916669   59268 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0509 13:35:04.943264   59268 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0509 13:35:04.958629   59268 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0509 13:35:04.972756   59268 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0509 13:35:04.972800   59268 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0509 13:35:04.988573   59268 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0509 13:35:04.988580   59268 kubeadm.go:157] found existing configuration files:

I0509 13:35:04.988641   59268 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0509 13:35:05.003109   59268 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0509 13:35:05.003177   59268 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0509 13:35:05.018270   59268 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0509 13:35:05.031978   59268 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0509 13:35:05.032029   59268 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0509 13:35:05.049909   59268 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0509 13:35:05.069274   59268 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0509 13:35:05.069322   59268 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0509 13:35:05.081889   59268 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0509 13:35:05.095095   59268 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0509 13:35:05.095152   59268 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0509 13:35:05.108446   59268 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.32.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0509 13:35:05.492871   59268 kubeadm.go:310] [init] Using Kubernetes version: v1.32.0
I0509 13:35:05.493024   59268 kubeadm.go:310] [preflight] Running pre-flight checks
I0509 13:35:05.626588   59268 kubeadm.go:310] [preflight] The system verification failed. Printing the output from the verification:
I0509 13:35:05.626672   59268 kubeadm.go:310] [0;37mKERNEL_VERSION[0m: [0;32m6.11.0-24-generic[0m
I0509 13:35:05.626716   59268 kubeadm.go:310] [0;37mOS[0m: [0;32mLinux[0m
I0509 13:35:05.626772   59268 kubeadm.go:310] [0;37mCGROUPS_CPU[0m: [0;32menabled[0m
I0509 13:35:05.626832   59268 kubeadm.go:310] [0;37mCGROUPS_CPUSET[0m: [0;32menabled[0m
I0509 13:35:05.627517   59268 kubeadm.go:310] [0;37mCGROUPS_DEVICES[0m: [0;32menabled[0m
I0509 13:35:05.627584   59268 kubeadm.go:310] [0;37mCGROUPS_FREEZER[0m: [0;32menabled[0m
I0509 13:35:05.627643   59268 kubeadm.go:310] [0;37mCGROUPS_MEMORY[0m: [0;32menabled[0m
I0509 13:35:05.627726   59268 kubeadm.go:310] [0;37mCGROUPS_PIDS[0m: [0;32menabled[0m
I0509 13:35:05.627788   59268 kubeadm.go:310] [0;37mCGROUPS_HUGETLB[0m: [0;32menabled[0m
I0509 13:35:05.627855   59268 kubeadm.go:310] [0;37mCGROUPS_IO[0m: [0;32menabled[0m
I0509 13:35:05.708109   59268 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0509 13:35:05.708194   59268 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0509 13:35:05.708275   59268 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0509 13:35:05.727995   59268 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0509 13:35:05.734083   59268 out.go:235]     ▪ Generating certificates and keys ...
I0509 13:35:05.734414   59268 kubeadm.go:310] [certs] Using existing ca certificate authority
I0509 13:35:05.734560   59268 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0509 13:35:05.797400   59268 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0509 13:35:05.856876   59268 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0509 13:35:06.097268   59268 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0509 13:35:06.310868   59268 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0509 13:35:06.505660   59268 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0509 13:35:06.505760   59268 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0509 13:35:06.596012   59268 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0509 13:35:06.596372   59268 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0509 13:35:06.669458   59268 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0509 13:35:06.834925   59268 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0509 13:35:06.926811   59268 kubeadm.go:310] [certs] Generating "sa" key and public key
I0509 13:35:06.926865   59268 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0509 13:35:07.025662   59268 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0509 13:35:07.151705   59268 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0509 13:35:07.308005   59268 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0509 13:35:07.510833   59268 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0509 13:35:07.605806   59268 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0509 13:35:07.606373   59268 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0509 13:35:07.624708   59268 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0509 13:35:07.638373   59268 out.go:235]     ▪ Booting up control plane ...
I0509 13:35:07.638755   59268 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0509 13:35:07.638824   59268 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0509 13:35:07.638882   59268 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0509 13:35:07.661563   59268 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0509 13:35:07.672246   59268 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0509 13:35:07.672288   59268 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0509 13:35:07.865512   59268 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0509 13:35:07.865759   59268 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0509 13:35:08.867283   59268 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 1.001791458s
I0509 13:35:08.867468   59268 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0509 13:35:16.872556   59268 kubeadm.go:310] [api-check] The API server is healthy after 8.003294147s
I0509 13:35:16.938810   59268 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0509 13:35:16.978503   59268 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0509 13:35:17.039221   59268 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0509 13:35:17.039601   59268 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0509 13:35:17.068282   59268 kubeadm.go:310] [bootstrap-token] Using token: p9301i.5srj6j4f8almaqab
I0509 13:35:17.076496   59268 out.go:235]     ▪ Configuring RBAC rules ...
I0509 13:35:17.077330   59268 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0509 13:35:17.093681   59268 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0509 13:35:17.105502   59268 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0509 13:35:17.113436   59268 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0509 13:35:17.120812   59268 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0509 13:35:17.129903   59268 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0509 13:35:17.282215   59268 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0509 13:35:17.758785   59268 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0509 13:35:18.328454   59268 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0509 13:35:18.335141   59268 kubeadm.go:310] 
I0509 13:35:18.342309   59268 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0509 13:35:18.342333   59268 kubeadm.go:310] 
I0509 13:35:18.342566   59268 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0509 13:35:18.342576   59268 kubeadm.go:310] 
I0509 13:35:18.342684   59268 kubeadm.go:310]   mkdir -p $HOME/.kube
I0509 13:35:18.342863   59268 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0509 13:35:18.343027   59268 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0509 13:35:18.343037   59268 kubeadm.go:310] 
I0509 13:35:18.343199   59268 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0509 13:35:18.343208   59268 kubeadm.go:310] 
I0509 13:35:18.343350   59268 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0509 13:35:18.343359   59268 kubeadm.go:310] 
I0509 13:35:18.346592   59268 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0509 13:35:18.346852   59268 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0509 13:35:18.347219   59268 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0509 13:35:18.347232   59268 kubeadm.go:310] 
I0509 13:35:18.347943   59268 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0509 13:35:18.348180   59268 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0509 13:35:18.348220   59268 kubeadm.go:310] 
I0509 13:35:18.348477   59268 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token p9301i.5srj6j4f8almaqab \
I0509 13:35:18.348791   59268 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:f2fa4c48d7cd46f74da32f0ae5a1de70d7c70ec4a0c4ccbb6c753e727a16e635 \
I0509 13:35:18.350892   59268 kubeadm.go:310] 	--control-plane 
I0509 13:35:18.351153   59268 kubeadm.go:310] 
I0509 13:35:18.352105   59268 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0509 13:35:18.352125   59268 kubeadm.go:310] 
I0509 13:35:18.352373   59268 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token p9301i.5srj6j4f8almaqab \
I0509 13:35:18.352704   59268 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:f2fa4c48d7cd46f74da32f0ae5a1de70d7c70ec4a0c4ccbb6c753e727a16e635 
I0509 13:35:18.359698   59268 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0509 13:35:18.360184   59268 kubeadm.go:310] 	[WARNING SystemVerification]: failed to parse kernel config: unable to load kernel module: "configs", output: "modprobe: FATAL: Module configs not found in directory /lib/modules/6.11.0-24-generic\n", err: exit status 1
I0509 13:35:18.360422   59268 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0509 13:35:18.360446   59268 cni.go:84] Creating CNI manager for ""
I0509 13:35:18.360467   59268 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0509 13:35:18.366771   59268 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0509 13:35:18.373495   59268 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0509 13:35:18.400058   59268 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0509 13:35:18.453513   59268 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0509 13:35:18.453603   59268 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_05_09T13_35_18_0700 minikube.k8s.io/version=v1.35.0 minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0509 13:35:18.453780   59268 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0509 13:35:18.939863   59268 ops.go:34] apiserver oom_adj: -16
I0509 13:35:18.943287   59268 kubeadm.go:1113] duration metric: took 489.548535ms to wait for elevateKubeSystemPrivileges
I0509 13:35:18.943320   59268 kubeadm.go:394] duration metric: took 14.02674801s to StartCluster
I0509 13:35:18.943345   59268 settings.go:142] acquiring lock: {Name:mka09f2f860f7b1d7a5aef10cec47c8400a8cff7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0509 13:35:18.943557   59268 settings.go:150] Updating kubeconfig:  /home/gomti-verma/.kube/config
I0509 13:35:18.946674   59268 lock.go:35] WriteFile acquiring /home/gomti-verma/.kube/config: {Name:mk7bb1e0d3c0e3b44d5c612757050dbfd1408a82 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0509 13:35:18.947519   59268 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0509 13:35:18.947533   59268 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0509 13:35:18.947677   59268 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0509 13:35:18.947800   59268 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0509 13:35:18.947861   59268 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I0509 13:35:18.947903   59268 host.go:66] Checking if "minikube" exists ...
I0509 13:35:18.948006   59268 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0509 13:35:18.948048   59268 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0509 13:35:18.948069   59268 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0509 13:35:18.948627   59268 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0509 13:35:18.948854   59268 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0509 13:35:18.957987   59268 out.go:177] 🔎  Verifying Kubernetes components...
I0509 13:35:18.964190   59268 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0509 13:35:19.053528   59268 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0509 13:35:19.064347   59268 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0509 13:35:19.064379   59268 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0509 13:35:19.065061   59268 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0509 13:35:19.089754   59268 addons.go:238] Setting addon default-storageclass=true in "minikube"
I0509 13:35:19.089807   59268 host.go:66] Checking if "minikube" exists ...
I0509 13:35:19.095185   59268 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0509 13:35:19.148697   59268 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0509 13:35:19.148721   59268 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0509 13:35:19.148791   59268 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0509 13:35:19.175019   59268 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/gomti-verma/.minikube/machines/minikube/id_rsa Username:docker}
I0509 13:35:19.179342   59268 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0509 13:35:19.191149   59268 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/gomti-verma/.minikube/machines/minikube/id_rsa Username:docker}
I0509 13:35:19.289983   59268 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0509 13:35:19.341843   59268 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0509 13:35:19.358927   59268 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0509 13:35:19.548536   59268 start.go:971] {"host.minikube.internal": 192.168.49.1} host record injected into CoreDNS's ConfigMap
I0509 13:35:19.549327   59268 api_server.go:52] waiting for apiserver process to appear ...
I0509 13:35:19.549367   59268 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0509 13:35:19.747597   59268 api_server.go:72] duration metric: took 800.01864ms to wait for apiserver process to appear ...
I0509 13:35:19.747617   59268 api_server.go:88] waiting for apiserver healthz status ...
I0509 13:35:19.747645   59268 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0509 13:35:19.777234   59268 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0509 13:35:19.780601   59268 api_server.go:141] control plane version: v1.32.0
I0509 13:35:19.780645   59268 api_server.go:131] duration metric: took 33.015308ms to wait for apiserver health ...
I0509 13:35:19.780660   59268 system_pods.go:43] waiting for kube-system pods to appear ...
I0509 13:35:19.790458   59268 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0509 13:35:19.798009   59268 addons.go:514] duration metric: took 850.332336ms for enable addons: enabled=[storage-provisioner default-storageclass]
I0509 13:35:19.803334   59268 system_pods.go:59] 5 kube-system pods found
I0509 13:35:19.803377   59268 system_pods.go:61] "etcd-minikube" [bc57657a-1f93-4827-8822-8dadb74cd9dc] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0509 13:35:19.803395   59268 system_pods.go:61] "kube-apiserver-minikube" [7bd5bf1c-4617-4ba7-919a-b7c6cab4ac0b] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0509 13:35:19.803411   59268 system_pods.go:61] "kube-controller-manager-minikube" [4575012d-47ac-4338-a64c-7f0e4152e655] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0509 13:35:19.803419   59268 system_pods.go:61] "kube-scheduler-minikube" [a8bfdaca-7f06-4acc-b311-2d74228eda4a] Running
I0509 13:35:19.803429   59268 system_pods.go:61] "storage-provisioner" [caa28bc7-6d47-4e9e-88c1-80021f6c645c] Pending: PodScheduled:Unschedulable (0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.)
I0509 13:35:19.803442   59268 system_pods.go:74] duration metric: took 22.77042ms to wait for pod list to return data ...
I0509 13:35:19.803463   59268 kubeadm.go:582] duration metric: took 855.889236ms to wait for: map[apiserver:true system_pods:true]
I0509 13:35:19.803484   59268 node_conditions.go:102] verifying NodePressure condition ...
I0509 13:35:19.810772   59268 node_conditions.go:122] node storage ephemeral capacity is 243937628Ki
I0509 13:35:19.810805   59268 node_conditions.go:123] node cpu capacity is 4
I0509 13:35:19.810826   59268 node_conditions.go:105] duration metric: took 7.33309ms to run NodePressure ...
I0509 13:35:19.810850   59268 start.go:241] waiting for startup goroutines ...
I0509 13:35:20.054162   59268 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0509 13:35:20.054205   59268 start.go:246] waiting for cluster config update ...
I0509 13:35:20.054231   59268 start.go:255] writing updated cluster config ...
I0509 13:35:20.054844   59268 ssh_runner.go:195] Run: rm -f paused
I0509 13:35:20.242274   59268 start.go:600] kubectl: 1.33.0, cluster: 1.32.0 (minor skew: 1)
I0509 13:35:20.248826   59268 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
May 09 08:05:01 minikube cri-dockerd[1492]: time="2025-05-09T08:05:01Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
May 09 08:05:01 minikube cri-dockerd[1492]: time="2025-05-09T08:05:01Z" level=info msg="Start cri-dockerd grpc backend"
May 09 08:05:01 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
May 09 08:05:09 minikube cri-dockerd[1492]: time="2025-05-09T08:05:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e5070c210b6c34775092145f82834b6e261aeaf3beb1ffcfdcc0ec4436a6d0da/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
May 09 08:05:09 minikube cri-dockerd[1492]: time="2025-05-09T08:05:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/621d3e78d45c92baa3db9ab9440b468dad6d7c57e3f297a6607568415cc22b65/resolv.conf as [nameserver 192.168.49.1 options trust-ad ndots:0 edns0]"
May 09 08:05:09 minikube cri-dockerd[1492]: time="2025-05-09T08:05:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/11c8540ae80cdd1ebc84048a05c7fdb5b9456862d8b0207ee23b84bf1455431a/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
May 09 08:05:09 minikube cri-dockerd[1492]: time="2025-05-09T08:05:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/fade05825493655a07f596196dda5c6d1bc3fdf4d2c20327824e6a7beb8355c2/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
May 09 08:05:23 minikube cri-dockerd[1492]: time="2025-05-09T08:05:23Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/17802c9f1553ded3afd62787ce614708501b4508ad7db8569845405457d8ee70/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
May 09 08:05:23 minikube cri-dockerd[1492]: time="2025-05-09T08:05:23Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/727ad770f2f7926080baf029af73da7edee52225eef65b6ee0c08fa12e3e704a/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
May 09 08:05:23 minikube cri-dockerd[1492]: time="2025-05-09T08:05:23Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a1f11dcee0e341cf2022f5d3043f571ac8bb186aa37932e1f41c09d104234c2a/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
May 09 08:05:27 minikube cri-dockerd[1492]: time="2025-05-09T08:05:27Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
May 09 08:05:55 minikube dockerd[1221]: time="2025-05-09T08:05:55.297268098Z" level=info msg="ignoring event" container=e49c2ca162c1fc6932ba73cb2f3b71bcdccc3d3e0b8ce15f2f7cb02048d80224 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 09 08:09:19 minikube cri-dockerd[1492]: time="2025-05-09T08:09:19Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/50111df25d2379576e684326541ee0d2b8913579b11332073db5eba789827f38/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 09 08:09:34 minikube cri-dockerd[1492]: time="2025-05-09T08:09:34Z" level=info msg="Pulling image mongo:5: d9802f032d67: Downloading [==============================>                    ]   16.8MB/27.51MB"
May 09 08:09:44 minikube cri-dockerd[1492]: time="2025-05-09T08:09:44Z" level=info msg="Pulling image mongo:5: d9802f032d67: Downloading [==============================================>    ]  25.76MB/27.51MB"
May 09 08:09:54 minikube cri-dockerd[1492]: time="2025-05-09T08:09:54Z" level=info msg="Pulling image mongo:5: a7a1a92aeac5: Downloading [========>                                          ]  42.61MB/243.4MB"
May 09 08:10:04 minikube cri-dockerd[1492]: time="2025-05-09T08:10:04Z" level=info msg="Pulling image mongo:5: a7a1a92aeac5: Downloading [=============>                                     ]  65.84MB/243.4MB"
May 09 08:10:12 minikube cri-dockerd[1492]: time="2025-05-09T08:10:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d5f6d92b2d429a291c390120e9f8e2bfca4dfa41ca307999010f5e2b772f63d3/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 09 08:10:12 minikube cri-dockerd[1492]: time="2025-05-09T08:10:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5bbd99b8928fcd10043bc688236a1c697007dd48d28749a9eb4dfea5c6fe7cb3/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 09 08:10:14 minikube cri-dockerd[1492]: time="2025-05-09T08:10:14Z" level=info msg="Pulling image mongo:5: a7a1a92aeac5: Downloading [==================>                                ]  90.15MB/243.4MB"
May 09 08:10:24 minikube cri-dockerd[1492]: time="2025-05-09T08:10:24Z" level=info msg="Pulling image mongo:5: a7a1a92aeac5: Downloading [=====================>                             ]  106.9MB/243.4MB"
May 09 08:10:34 minikube cri-dockerd[1492]: time="2025-05-09T08:10:34Z" level=info msg="Pulling image mongo:5: a7a1a92aeac5: Downloading [=========================>                         ]    123MB/243.4MB"
May 09 08:10:36 minikube cri-dockerd[1492]: time="2025-05-09T08:10:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2c6b5db4c72d9ce8b43f7dad9fec6ea713d729e68a7669cff2d928341f34dfec/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 09 08:10:36 minikube cri-dockerd[1492]: time="2025-05-09T08:10:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bcdf06e533ff535e6831158f476b5546a8d7b9b5069eba65dc2e9790047abf87/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 09 08:10:44 minikube cri-dockerd[1492]: time="2025-05-09T08:10:44Z" level=info msg="Pulling image mongo:5: a7a1a92aeac5: Downloading [=============================>                     ]  144.1MB/243.4MB"
May 09 08:10:54 minikube cri-dockerd[1492]: time="2025-05-09T08:10:54Z" level=info msg="Pulling image mongo:5: a7a1a92aeac5: Downloading [==================================>                ]  169.5MB/243.4MB"
May 09 08:11:04 minikube cri-dockerd[1492]: time="2025-05-09T08:11:04Z" level=info msg="Pulling image mongo:5: a7a1a92aeac5: Downloading [=========================================>         ]  203.5MB/243.4MB"
May 09 08:11:14 minikube cri-dockerd[1492]: time="2025-05-09T08:11:14Z" level=info msg="Pulling image mongo:5: a7a1a92aeac5: Downloading [=============================================>     ]  222.3MB/243.4MB"
May 09 08:11:24 minikube cri-dockerd[1492]: time="2025-05-09T08:11:24Z" level=info msg="Pulling image mongo:5: a7a1a92aeac5: Extracting [===>                                               ]  15.04MB/243.4MB"
May 09 08:11:34 minikube cri-dockerd[1492]: time="2025-05-09T08:11:34Z" level=info msg="Pulling image mongo:5: a7a1a92aeac5: Extracting [======================================>            ]  189.4MB/243.4MB"
May 09 08:11:40 minikube cri-dockerd[1492]: time="2025-05-09T08:11:40Z" level=info msg="Stop pulling image mongo:5: Status: Downloaded newer image for mongo:5"
May 09 08:11:44 minikube dockerd[1221]: time="2025-05-09T08:11:44.812425135Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 09 08:11:44 minikube dockerd[1221]: time="2025-05-09T08:11:44.817168096Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 09 08:11:49 minikube dockerd[1221]: time="2025-05-09T08:11:49.732047019Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 09 08:11:49 minikube dockerd[1221]: time="2025-05-09T08:11:49.732431972Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 09 08:11:55 minikube dockerd[1221]: time="2025-05-09T08:11:55.043049489Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 09 08:11:55 minikube dockerd[1221]: time="2025-05-09T08:11:55.043701212Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 09 08:11:59 minikube dockerd[1221]: time="2025-05-09T08:11:59.973107642Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 09 08:11:59 minikube dockerd[1221]: time="2025-05-09T08:11:59.973205973Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 09 08:12:04 minikube dockerd[1221]: time="2025-05-09T08:12:04.973348059Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 09 08:12:04 minikube dockerd[1221]: time="2025-05-09T08:12:04.973446696Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 09 08:12:09 minikube dockerd[1221]: time="2025-05-09T08:12:09.172311925Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 09 08:12:09 minikube dockerd[1221]: time="2025-05-09T08:12:09.172397941Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 09 08:12:15 minikube dockerd[1221]: time="2025-05-09T08:12:15.212426929Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 09 08:12:15 minikube dockerd[1221]: time="2025-05-09T08:12:15.212514458Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 09 08:12:19 minikube dockerd[1221]: time="2025-05-09T08:12:19.514869606Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 09 08:12:19 minikube dockerd[1221]: time="2025-05-09T08:12:19.514982712Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 09 08:12:35 minikube dockerd[1221]: time="2025-05-09T08:12:35.485107091Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 09 08:12:35 minikube dockerd[1221]: time="2025-05-09T08:12:35.486041845Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 09 08:12:43 minikube dockerd[1221]: time="2025-05-09T08:12:43.165741258Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 09 08:12:43 minikube dockerd[1221]: time="2025-05-09T08:12:43.165826178Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 09 08:12:47 minikube dockerd[1221]: time="2025-05-09T08:12:47.172902646Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 09 08:12:47 minikube dockerd[1221]: time="2025-05-09T08:12:47.173059272Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 09 08:12:52 minikube dockerd[1221]: time="2025-05-09T08:12:52.277704517Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 09 08:12:52 minikube dockerd[1221]: time="2025-05-09T08:12:52.277738555Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 09 08:13:22 minikube dockerd[1221]: time="2025-05-09T08:13:22.078408706Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 09 08:13:22 minikube dockerd[1221]: time="2025-05-09T08:13:22.078504590Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 09 08:13:47 minikube dockerd[1221]: time="2025-05-09T08:13:47.673623155Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving"
May 09 08:13:47 minikube dockerd[1221]: time="2025-05-09T08:13:47.673695658Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving"
May 09 08:13:47 minikube dockerd[1221]: time="2025-05-09T08:13:47.681517896Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving"


==> container status <==
CONTAINER           IMAGE                                                                           CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
ea83d411e0853       mongo@sha256:54bcd8da3ea5eec561b68c605046c55c6b304387dc4c2bf5b3a5f5064fbb7495   2 minutes ago       Running             mongodb                   0                   50111df25d237       mongodb-68c9fdc8cf-wpzz7
62aff10293561       6e38f40d628db                                                                   7 minutes ago       Running             storage-provisioner       1                   a1f11dcee0e34       storage-provisioner
2c7d98fc74a96       c69fa2e9cbf5f                                                                   8 minutes ago       Running             coredns                   0                   727ad770f2f79       coredns-668d6bf9bc-x79sj
e49c2ca162c1f       6e38f40d628db                                                                   8 minutes ago       Exited              storage-provisioner       0                   a1f11dcee0e34       storage-provisioner
7473d82fd990e       040f9f8aac8cd                                                                   8 minutes ago       Running             kube-proxy                0                   17802c9f1553d       kube-proxy-k6859
15904d0d335aa       a9e7e6b294baf                                                                   8 minutes ago       Running             etcd                      0                   fade058254936       etcd-minikube
d9695b28ce381       8cab3d2a8bd0f                                                                   8 minutes ago       Running             kube-controller-manager   0                   11c8540ae80cd       kube-controller-manager-minikube
9953da93e5825       a389e107f4ff1                                                                   8 minutes ago       Running             kube-scheduler            0                   621d3e78d45c9       kube-scheduler-minikube
35431574b60a9       c2e17b8d0f4a3                                                                   8 minutes ago       Running             kube-apiserver            0                   e5070c210b6c3       kube-apiserver-minikube


==> coredns [2c7d98fc74a9] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:35476 - 6563 "HINFO IN 3147372269269397227.128613926200937472. udp 56 false 512" NXDOMAIN qr,rd,ra 131 0.077185651s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[95598215]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (09-May-2025 08:05:25.868) (total time: 30109ms):
Trace[95598215]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30006ms (08:05:55.874)
Trace[95598215]: [30.109571362s] [30.109571362s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[572044188]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (09-May-2025 08:05:25.868) (total time: 30109ms):
Trace[572044188]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30006ms (08:05:55.874)
Trace[572044188]: [30.109471432s] [30.109471432s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1187820759]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (09-May-2025 08:05:25.868) (total time: 30109ms):
Trace[1187820759]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30006ms (08:05:55.874)
Trace[1187820759]: [30.109916442s] [30.109916442s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_05_09T13_35_18_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 09 May 2025 08:05:14 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 09 May 2025 08:13:48 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 09 May 2025 08:11:55 +0000   Fri, 09 May 2025 08:05:11 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 09 May 2025 08:11:55 +0000   Fri, 09 May 2025 08:05:11 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 09 May 2025 08:11:55 +0000   Fri, 09 May 2025 08:05:11 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 09 May 2025 08:11:55 +0000   Fri, 09 May 2025 08:05:14 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  243937628Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8009368Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  243937628Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8009368Ki
  pods:               110
System Info:
  Machine ID:                 462c125f092a4b93886642313bb02bbf
  System UUID:                64f746bd-7745-4daa-b8be-cca206d39591
  Boot ID:                    3f3750e8-aca6-4e14-90e4-a476f3a02bc6
  Kernel Version:             6.11.0-24-generic
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (12 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     backend-fb67664f-2xbng              0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m42s
  default                     backend-fb67664f-dtwpj              0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m42s
  default                     frontend-cf954c9f7-6dt5k            0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m18s
  default                     frontend-cf954c9f7-wdrch            0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m18s
  default                     mongodb-68c9fdc8cf-wpzz7            0 (0%)        0 (0%)      0 (0%)           0 (0%)         4m35s
  kube-system                 coredns-668d6bf9bc-x79sj            100m (2%)     0 (0%)      70Mi (0%)        170Mi (2%)     8m31s
  kube-system                 etcd-minikube                       100m (2%)     0 (0%)      100Mi (1%)       0 (0%)         8m36s
  kube-system                 kube-apiserver-minikube             250m (6%)     0 (0%)      0 (0%)           0 (0%)         8m36s
  kube-system                 kube-controller-manager-minikube    200m (5%)     0 (0%)      0 (0%)           0 (0%)         8m37s
  kube-system                 kube-proxy-k6859                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         8m31s
  kube-system                 kube-scheduler-minikube             100m (2%)     0 (0%)      0 (0%)           0 (0%)         8m36s
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         8m34s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (18%)  0 (0%)
  memory             170Mi (2%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  Starting                 8m27s                  kube-proxy       
  Normal  NodeHasSufficientMemory  8m45s (x8 over 8m45s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    8m45s (x8 over 8m45s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     8m45s (x7 over 8m45s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  8m45s                  kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 8m36s                  kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  8m36s                  kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  8m36s                  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    8m36s                  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     8m36s                  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           8m32s                  node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[May 9 06:20] x86/cpu: SGX disabled by BIOS.
[  +0.355972] hpet_acpi_add: no address or irqs in _CRS
[  +0.049429] i8042: PNP: PS/2 appears to have AUX port disabled, if this is incorrect please boot with i8042.nopnp
[  +0.000265] i8042: Warning: Keylock active
[  +0.001988] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.000189] platform eisa.0: EISA: Cannot allocate resource for mainboard
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 1
[  +0.000003] platform eisa.0: Cannot allocate resource for EISA slot 2
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 3
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 4
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 5
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 6
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 7
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 8
[  +0.040370] ENERGY_PERF_BIAS: Set to 'normal', was 'performance'
[  +0.356628] wmi_bus wmi_bus-PNP0C14:03: [Firmware Bug]: WQBC data block query control method not found
[  +2.673333] resource: resource sanity check: requesting [mem 0x00000000fdffe800-0x00000000fe0007ff], which spans more than pnp 00:06 [mem 0xfdb00000-0xfdffffff]
[  +0.000007] caller get_primary_reg_base+0x4f/0xb0 [intel_pmc_core] mapping multiple BARs
[  +1.106403] nvidia: loading out-of-tree module taints kernel.
[  +0.000009] nvidia: module license 'NVIDIA' taints kernel.
[  +0.000001] Disabling lock debugging due to kernel taint
[  +0.000004] nvidia: module license taints kernel.

[  +0.451756] NVRM: loading NVIDIA UNIX x86_64 Kernel Module  550.120  Fri Sep 13 10:10:01 UTC 2024
[  +0.217194] kauditd_printk_skb: 153 callbacks suppressed
[  +0.017334] ACPI Warning: \_SB.PCI0.PEG1.PEGP._DSM: Argument #4 type mismatch - Found [Buffer], ACPI requires [Package] (20240322/nsarguments-61)
[  +1.104083] nvidia_uvm: module uses symbols nvUvmInterfaceDisableAccessCntr from proprietary module nvidia, inheriting taint.
[ +48.152934] warning: `ThreadPoolForeg' uses wireless extensions which will stop working for Wi-Fi 7 hardware; use nl80211
[May 9 06:24] workqueue: delayed_fput hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUND
[  +2.924613] workqueue: delayed_fput hogged CPU for >10000us 5 times, consider switching to WQ_UNBOUND
[ +30.083532] workqueue: delayed_fput hogged CPU for >10000us 7 times, consider switching to WQ_UNBOUND
[May 9 06:30] kauditd_printk_skb: 4 callbacks suppressed
[May 9 06:50] show_signal_msg: 59 callbacks suppressed
[May 9 08:11] workqueue: delayed_fput hogged CPU for >10000us 11 times, consider switching to WQ_UNBOUND


==> etcd [15904d0d335a] <==
{"level":"warn","ts":"2025-05-09T08:05:11.061178Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-05-09T08:05:11.071879Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2025-05-09T08:05:11.073061Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-05-09T08:05:11.073505Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-05-09T08:05:11.073545Z","caller":"embed/etcd.go:497","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-05-09T08:05:11.081762Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-05-09T08:05:11.082239Z","caller":"embed/etcd.go:311","msg":"starting an etcd server","etcd-version":"3.5.16","git-sha":"f20bbad","go-version":"go1.22.7","go-os":"linux","go-arch":"amd64","max-cpu-set":4,"max-cpu-available":4,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.49.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-05-09T08:05:11.105254Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"19.244994ms"}
{"level":"info","ts":"2025-05-09T08:05:11.141581Z","caller":"etcdserver/raft.go:505","msg":"starting local member","local-member-id":"aec36adc501070cc","cluster-id":"fa54960ea34d58be"}
{"level":"info","ts":"2025-05-09T08:05:11.150814Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2025-05-09T08:05:11.154837Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 0"}
{"level":"info","ts":"2025-05-09T08:05:11.154878Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2025-05-09T08:05:11.154913Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2025-05-09T08:05:11.155046Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2025-05-09T08:05:11.200329Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-05-09T08:05:11.218124Z","caller":"mvcc/kvstore.go:423","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2025-05-09T08:05:11.229334Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-05-09T08:05:11.243623Z","caller":"etcdserver/server.go:873","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.16","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-05-09T08:05:11.244631Z","caller":"etcdserver/server.go:757","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-05-09T08:05:11.247427Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-05-09T08:05:11.247544Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-05-09T08:05:11.247558Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-05-09T08:05:11.251034Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-05-09T08:05:11.252349Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-05-09T08:05:11.249192Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-05-09T08:05:11.258867Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-05-09T08:05:11.259151Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-05-09T08:05:11.258625Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-05-09T08:05:11.270641Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-05-09T08:05:11.271169Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-05-09T08:05:11.956349Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2025-05-09T08:05:11.956423Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2025-05-09T08:05:11.956440Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2025-05-09T08:05:11.956468Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2025-05-09T08:05:11.956491Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-05-09T08:05:11.956499Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2025-05-09T08:05:11.956507Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-05-09T08:05:11.958793Z","caller":"etcdserver/server.go:2651","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2025-05-09T08:05:11.961079Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-05-09T08:05:11.961924Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-05-09T08:05:11.961997Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-05-09T08:05:11.962120Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-05-09T08:05:11.965448Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-05-09T08:05:11.965574Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-05-09T08:05:11.965637Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-05-09T08:05:11.965656Z","caller":"etcdserver/server.go:2675","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2025-05-09T08:05:11.971692Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-05-09T08:05:11.975005Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-05-09T08:05:11.976504Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-05-09T08:05:11.976970Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-05-09T08:05:15.094338Z","caller":"traceutil/trace.go:171","msg":"trace[2104948430] transaction","detail":"{read_only:false; response_revision:106; number_of_response:1; }","duration":"101.830721ms","start":"2025-05-09T08:05:14.992490Z","end":"2025-05-09T08:05:15.094321Z","steps":["trace[2104948430] 'process raft request'  (duration: 101.46088ms)"],"step_count":1}
{"level":"info","ts":"2025-05-09T08:05:21.624276Z","caller":"traceutil/trace.go:171","msg":"trace[1778402349] transaction","detail":"{read_only:false; response_revision:334; number_of_response:1; }","duration":"127.091144ms","start":"2025-05-09T08:05:21.497156Z","end":"2025-05-09T08:05:21.624247Z","steps":["trace[1778402349] 'process raft request'  (duration: 126.907021ms)"],"step_count":1}
{"level":"info","ts":"2025-05-09T08:11:26.605155Z","caller":"traceutil/trace.go:171","msg":"trace[470840761] transaction","detail":"{read_only:false; response_revision:761; number_of_response:1; }","duration":"105.717066ms","start":"2025-05-09T08:11:26.499405Z","end":"2025-05-09T08:11:26.605122Z","steps":["trace[470840761] 'process raft request'  (duration: 67.294696ms)","trace[470840761] 'compare'  (duration: 38.251945ms)"],"step_count":2}


==> kernel <==
 08:13:54 up  1:53,  0 users,  load average: 2.40, 2.46, 2.53
Linux minikube 6.11.0-24-generic #24~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Mar 25 20:14:34 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [35431574b60a] <==
I0509 08:05:13.985215       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0509 08:05:13.986041       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0509 08:05:13.986059       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0509 08:05:13.986092       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0509 08:05:13.986112       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0509 08:05:13.986471       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0509 08:05:13.986496       1 controller.go:78] Starting OpenAPI AggregationController
I0509 08:05:13.987307       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0509 08:05:13.987944       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0509 08:05:13.987958       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0509 08:05:13.987974       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0509 08:05:13.987978       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0509 08:05:13.991199       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0509 08:05:13.991215       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0509 08:05:13.994011       1 controller.go:142] Starting OpenAPI controller
I0509 08:05:13.994065       1 controller.go:90] Starting OpenAPI V3 controller
I0509 08:05:13.994082       1 naming_controller.go:294] Starting NamingConditionController
I0509 08:05:13.994106       1 establishing_controller.go:81] Starting EstablishingController
I0509 08:05:13.994120       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0509 08:05:13.994298       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0509 08:05:13.994339       1 crd_finalizer.go:269] Starting CRDFinalizer
I0509 08:05:13.994393       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0509 08:05:13.994500       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0509 08:05:14.067100       1 shared_informer.go:320] Caches are synced for node_authorizer
I0509 08:05:14.076747       1 cache.go:39] Caches are synced for LocalAvailability controller
I0509 08:05:14.076799       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0509 08:05:14.076815       1 policy_source.go:240] refreshing policies
I0509 08:05:14.085028       1 shared_informer.go:320] Caches are synced for configmaps
I0509 08:05:14.087249       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0509 08:05:14.087715       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0509 08:05:14.087726       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0509 08:05:14.087988       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0509 08:05:14.088009       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0509 08:05:14.088034       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0509 08:05:14.098131       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0509 08:05:14.098667       1 aggregator.go:171] initial CRD sync complete...
I0509 08:05:14.099277       1 autoregister_controller.go:144] Starting autoregister controller
I0509 08:05:14.099289       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0509 08:05:14.099297       1 cache.go:39] Caches are synced for autoregister controller
I0509 08:05:14.103885       1 controller.go:615] quota admission added evaluator for: namespaces
E0509 08:05:14.114167       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I0509 08:05:14.333215       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0509 08:05:15.095806       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0509 08:05:15.106727       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0509 08:05:15.106759       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0509 08:05:16.221696       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0509 08:05:16.297664       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0509 08:05:16.408970       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0509 08:05:16.429121       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0509 08:05:16.429976       1 controller.go:615] quota admission added evaluator for: endpoints
I0509 08:05:16.436857       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0509 08:05:17.068486       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0509 08:05:17.712291       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0509 08:05:17.756448       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0509 08:05:17.771268       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0509 08:05:21.758002       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0509 08:05:22.610304       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0509 08:09:58.773757       1 alloc.go:330] "allocated clusterIPs" service="default/mongodb" clusterIPs={"IPv4":"10.111.0.208"}
I0509 08:10:22.814675       1 alloc.go:330] "allocated clusterIPs" service="default/backend" clusterIPs={"IPv4":"10.99.43.98"}
I0509 08:10:45.298365       1 alloc.go:330] "allocated clusterIPs" service="default/frontend" clusterIPs={"IPv4":"10.109.148.56"}


==> kube-controller-manager [d9695b28ce38] <==
I0509 08:05:21.791338       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0509 08:05:21.791402       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0509 08:05:21.791450       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0509 08:05:21.792715       1 shared_informer.go:320] Caches are synced for garbage collector
I0509 08:05:21.797351       1 shared_informer.go:320] Caches are synced for garbage collector
I0509 08:05:21.797404       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0509 08:05:21.797426       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0509 08:05:22.084505       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0509 08:05:22.590975       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="811.424003ms"
I0509 08:05:22.612451       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="20.950914ms"
I0509 08:05:22.613670       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="111.252µs"
I0509 08:05:22.629443       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="97.277µs"
I0509 08:05:22.724077       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0509 08:05:24.937206       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="59.204µs"
I0509 08:05:28.010106       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0509 08:06:06.909431       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="32.601799ms"
I0509 08:06:06.909637       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="128.264µs"
I0509 08:09:18.224484       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongodb-68c9fdc8cf" duration="73.141797ms"
I0509 08:09:18.237784       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongodb-68c9fdc8cf" duration="13.231421ms"
I0509 08:09:18.299571       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongodb-68c9fdc8cf" duration="61.727872ms"
I0509 08:09:18.299730       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongodb-68c9fdc8cf" duration="87.515µs"
I0509 08:10:11.414127       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/backend-fb67664f" duration="51.092527ms"
I0509 08:10:11.434927       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/backend-fb67664f" duration="19.637212ms"
I0509 08:10:11.435743       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/backend-fb67664f" duration="782.286µs"
I0509 08:10:11.438201       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/backend-fb67664f" duration="132.846µs"
I0509 08:10:11.453894       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/backend-fb67664f" duration="36.786µs"
I0509 08:10:35.413450       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/frontend-cf954c9f7" duration="66.251301ms"
I0509 08:10:35.433241       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/frontend-cf954c9f7" duration="19.744974ms"
I0509 08:10:35.460480       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/frontend-cf954c9f7" duration="26.837667ms"
I0509 08:10:35.461121       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/frontend-cf954c9f7" duration="310.311µs"
I0509 08:11:35.451373       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0509 08:11:41.048708       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongodb-68c9fdc8cf" duration="20.722501ms"
I0509 08:11:41.048984       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongodb-68c9fdc8cf" duration="244.835µs"
I0509 08:11:45.105673       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/backend-fb67664f" duration="100.509µs"
I0509 08:11:50.156306       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/backend-fb67664f" duration="104.318µs"
I0509 08:11:55.233302       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/frontend-cf954c9f7" duration="112.025µs"
I0509 08:11:55.770368       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0509 08:11:59.667136       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/backend-fb67664f" duration="98.426µs"
I0509 08:12:00.303818       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/frontend-cf954c9f7" duration="88.318µs"
I0509 08:12:04.663178       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/backend-fb67664f" duration="86.451µs"
I0509 08:12:06.661449       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/frontend-cf954c9f7" duration="97.334µs"
I0509 08:12:11.660160       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/frontend-cf954c9f7" duration="92.157µs"
I0509 08:12:17.664400       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/backend-fb67664f" duration="129.817µs"
I0509 08:12:24.659289       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/backend-fb67664f" duration="88.351µs"
I0509 08:12:28.661392       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/frontend-cf954c9f7" duration="88.816µs"
I0509 08:12:30.660146       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/backend-fb67664f" duration="90.406µs"
I0509 08:12:32.659573       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/frontend-cf954c9f7" duration="93.701µs"
I0509 08:12:38.661428       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/backend-fb67664f" duration="86.84µs"
I0509 08:12:39.674453       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/frontend-cf954c9f7" duration="85.945µs"
I0509 08:12:47.688189       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/backend-fb67664f" duration="90.91µs"
I0509 08:12:47.713992       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/frontend-cf954c9f7" duration="130.207µs"
I0509 08:12:58.660561       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/backend-fb67664f" duration="86.255µs"
I0509 08:13:02.661453       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/backend-fb67664f" duration="94.236µs"
I0509 08:13:02.685477       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/frontend-cf954c9f7" duration="92.71µs"
I0509 08:13:05.665431       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/frontend-cf954c9f7" duration="87.926µs"
I0509 08:13:09.667971       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/backend-fb67664f" duration="117.237µs"
I0509 08:13:16.668443       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/frontend-cf954c9f7" duration="148.103µs"
I0509 08:13:17.688668       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/frontend-cf954c9f7" duration="88.253µs"
I0509 08:13:35.657426       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/backend-fb67664f" duration="93.796µs"
I0509 08:13:46.683792       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/backend-fb67664f" duration="93.766µs"


==> kube-proxy [7473d82fd990] <==
I0509 08:05:25.938382       1 server_linux.go:66] "Using iptables proxy"
I0509 08:05:26.227624       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0509 08:05:26.227832       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0509 08:05:26.318031       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0509 08:05:26.318130       1 server_linux.go:170] "Using iptables Proxier"
I0509 08:05:26.327008       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0509 08:05:26.358679       1 server.go:497] "Version info" version="v1.32.0"
I0509 08:05:26.358727       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0509 08:05:26.371587       1 config.go:199] "Starting service config controller"
I0509 08:05:26.372130       1 shared_informer.go:313] Waiting for caches to sync for service config
I0509 08:05:26.372480       1 config.go:105] "Starting endpoint slice config controller"
I0509 08:05:26.372862       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0509 08:05:26.374734       1 config.go:329] "Starting node config controller"
I0509 08:05:26.375260       1 shared_informer.go:313] Waiting for caches to sync for node config
I0509 08:05:26.472893       1 shared_informer.go:320] Caches are synced for service config
I0509 08:05:26.473968       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0509 08:05:26.476005       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [9953da93e582] <==
E0509 08:05:14.103684       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0509 08:05:14.105028       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0509 08:05:14.112534       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0509 08:05:14.106432       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0509 08:05:14.114883       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0509 08:05:14.106486       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0509 08:05:14.115103       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0509 08:05:14.106533       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0509 08:05:14.115129       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0509 08:05:14.115779       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0509 08:05:14.116786       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0509 08:05:14.116253       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0509 08:05:14.117684       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0509 08:05:14.117775       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0509 08:05:14.116357       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0509 08:05:14.117991       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0509 08:05:14.116409       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0509 08:05:14.118017       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0509 08:05:14.116465       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0509 08:05:14.118043       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0509 08:05:14.116511       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0509 08:05:14.118063       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E0509 08:05:14.118618       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0509 08:05:14.119128       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0509 08:05:14.119292       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0509 08:05:14.121235       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0509 08:05:14.121985       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0509 08:05:14.938160       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0509 08:05:14.938226       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0509 08:05:15.015866       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0509 08:05:15.016024       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0509 08:05:15.041048       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0509 08:05:15.041095       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0509 08:05:15.079404       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0509 08:05:15.079448       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0509 08:05:15.130477       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0509 08:05:15.130545       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0509 08:05:15.221808       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0509 08:05:15.221858       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0509 08:05:15.226556       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0509 08:05:15.226721       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0509 08:05:15.233132       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0509 08:05:15.233179       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0509 08:05:15.255231       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0509 08:05:15.255284       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0509 08:05:15.334248       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0509 08:05:15.334716       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0509 08:05:15.448435       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0509 08:05:15.448592       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0509 08:05:15.452994       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0509 08:05:15.453138       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0509 08:05:15.580817       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0509 08:05:15.580856       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0509 08:05:15.610375       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0509 08:05:15.610408       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0509 08:05:15.643708       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0509 08:05:15.643744       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0509 08:05:15.683105       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0509 08:05:15.683138       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
I0509 08:05:18.188996       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
May 09 08:11:59 minikube kubelet[2291]: E0509 08:11:59.982097    2291 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="frontend:latest"
May 09 08:11:59 minikube kubelet[2291]: E0509 08:11:59.982199    2291 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="frontend:latest"
May 09 08:11:59 minikube kubelet[2291]: E0509 08:11:59.982592    2291 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:frontend,Image:frontend:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bvbh5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod frontend-cf954c9f7-6dt5k_default(f938c8ca-8ed8-4431-9ee1-0906001d2e5e): ErrImagePull: Error response from daemon: pull access denied for frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 09 08:11:59 minikube kubelet[2291]: E0509 08:11:59.984116    2291 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend\" with ErrImagePull: \"Error response from daemon: pull access denied for frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/frontend-cf954c9f7-6dt5k" podUID="f938c8ca-8ed8-4431-9ee1-0906001d2e5e"
May 09 08:12:00 minikube kubelet[2291]: E0509 08:12:00.282051    2291 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend\" with ImagePullBackOff: \"Back-off pulling image \\\"frontend:latest\\\": ErrImagePull: Error response from daemon: pull access denied for frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/frontend-cf954c9f7-6dt5k" podUID="f938c8ca-8ed8-4431-9ee1-0906001d2e5e"
May 09 08:12:04 minikube kubelet[2291]: E0509 08:12:04.982975    2291 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="backend:latest"
May 09 08:12:04 minikube kubelet[2291]: E0509 08:12:04.983079    2291 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="backend:latest"
May 09 08:12:04 minikube kubelet[2291]: E0509 08:12:04.983386    2291 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:backend,Image:backend:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:NODE_ENV,Value:production,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sn52z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod backend-fb67664f-dtwpj_default(34a4327a-9c1d-48a8-b573-300ba65fd4fa): ErrImagePull: Error response from daemon: pull access denied for backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 09 08:12:04 minikube kubelet[2291]: E0509 08:12:04.985408    2291 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend\" with ErrImagePull: \"Error response from daemon: pull access denied for backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/backend-fb67664f-dtwpj" podUID="34a4327a-9c1d-48a8-b573-300ba65fd4fa"
May 09 08:12:09 minikube kubelet[2291]: E0509 08:12:09.181654    2291 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="backend:latest"
May 09 08:12:09 minikube kubelet[2291]: E0509 08:12:09.181747    2291 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="backend:latest"
May 09 08:12:09 minikube kubelet[2291]: E0509 08:12:09.182110    2291 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:backend,Image:backend:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:NODE_ENV,Value:production,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s26z9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod backend-fb67664f-2xbng_default(db74514b-856a-4170-8331-48cbdc6f92c2): ErrImagePull: Error response from daemon: pull access denied for backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 09 08:12:09 minikube kubelet[2291]: E0509 08:12:09.183980    2291 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend\" with ErrImagePull: \"Error response from daemon: pull access denied for backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/backend-fb67664f-2xbng" podUID="db74514b-856a-4170-8331-48cbdc6f92c2"
May 09 08:12:15 minikube kubelet[2291]: E0509 08:12:15.221015    2291 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="frontend:latest"
May 09 08:12:15 minikube kubelet[2291]: E0509 08:12:15.221136    2291 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="frontend:latest"
May 09 08:12:15 minikube kubelet[2291]: E0509 08:12:15.221467    2291 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:frontend,Image:frontend:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rdlmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod frontend-cf954c9f7-wdrch_default(c80782cc-b26e-4324-8228-1a0a179cc5a8): ErrImagePull: Error response from daemon: pull access denied for frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 09 08:12:15 minikube kubelet[2291]: E0509 08:12:15.223022    2291 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend\" with ErrImagePull: \"Error response from daemon: pull access denied for frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/frontend-cf954c9f7-wdrch" podUID="c80782cc-b26e-4324-8228-1a0a179cc5a8"
May 09 08:12:17 minikube kubelet[2291]: E0509 08:12:17.638112    2291 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend\" with ImagePullBackOff: \"Back-off pulling image \\\"backend:latest\\\": ErrImagePull: Error response from daemon: pull access denied for backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/backend-fb67664f-dtwpj" podUID="34a4327a-9c1d-48a8-b573-300ba65fd4fa"
May 09 08:12:19 minikube kubelet[2291]: E0509 08:12:19.524504    2291 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="frontend:latest"
May 09 08:12:19 minikube kubelet[2291]: E0509 08:12:19.524612    2291 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="frontend:latest"
May 09 08:12:19 minikube kubelet[2291]: E0509 08:12:19.524802    2291 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:frontend,Image:frontend:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bvbh5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod frontend-cf954c9f7-6dt5k_default(f938c8ca-8ed8-4431-9ee1-0906001d2e5e): ErrImagePull: Error response from daemon: pull access denied for frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 09 08:12:19 minikube kubelet[2291]: E0509 08:12:19.526406    2291 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend\" with ErrImagePull: \"Error response from daemon: pull access denied for frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/frontend-cf954c9f7-6dt5k" podUID="f938c8ca-8ed8-4431-9ee1-0906001d2e5e"
May 09 08:12:24 minikube kubelet[2291]: E0509 08:12:24.636849    2291 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend\" with ImagePullBackOff: \"Back-off pulling image \\\"backend:latest\\\": ErrImagePull: Error response from daemon: pull access denied for backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/backend-fb67664f-2xbng" podUID="db74514b-856a-4170-8331-48cbdc6f92c2"
May 09 08:12:28 minikube kubelet[2291]: E0509 08:12:28.637560    2291 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend\" with ImagePullBackOff: \"Back-off pulling image \\\"frontend:latest\\\": ErrImagePull: Error response from daemon: pull access denied for frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/frontend-cf954c9f7-wdrch" podUID="c80782cc-b26e-4324-8228-1a0a179cc5a8"
May 09 08:12:32 minikube kubelet[2291]: E0509 08:12:32.637574    2291 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend\" with ImagePullBackOff: \"Back-off pulling image \\\"frontend:latest\\\": ErrImagePull: Error response from daemon: pull access denied for frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/frontend-cf954c9f7-6dt5k" podUID="f938c8ca-8ed8-4431-9ee1-0906001d2e5e"
May 09 08:12:35 minikube kubelet[2291]: E0509 08:12:35.495775    2291 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="backend:latest"
May 09 08:12:35 minikube kubelet[2291]: E0509 08:12:35.495873    2291 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="backend:latest"
May 09 08:12:35 minikube kubelet[2291]: E0509 08:12:35.496121    2291 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:backend,Image:backend:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:NODE_ENV,Value:production,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sn52z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod backend-fb67664f-dtwpj_default(34a4327a-9c1d-48a8-b573-300ba65fd4fa): ErrImagePull: Error response from daemon: pull access denied for backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 09 08:12:35 minikube kubelet[2291]: E0509 08:12:35.497540    2291 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend\" with ErrImagePull: \"Error response from daemon: pull access denied for backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/backend-fb67664f-dtwpj" podUID="34a4327a-9c1d-48a8-b573-300ba65fd4fa"
May 09 08:12:43 minikube kubelet[2291]: E0509 08:12:43.174614    2291 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="backend:latest"
May 09 08:12:43 minikube kubelet[2291]: E0509 08:12:43.174726    2291 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="backend:latest"
May 09 08:12:43 minikube kubelet[2291]: E0509 08:12:43.175415    2291 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:backend,Image:backend:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:NODE_ENV,Value:production,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s26z9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod backend-fb67664f-2xbng_default(db74514b-856a-4170-8331-48cbdc6f92c2): ErrImagePull: Error response from daemon: pull access denied for backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 09 08:12:43 minikube kubelet[2291]: E0509 08:12:43.177133    2291 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend\" with ErrImagePull: \"Error response from daemon: pull access denied for backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/backend-fb67664f-2xbng" podUID="db74514b-856a-4170-8331-48cbdc6f92c2"
May 09 08:12:47 minikube kubelet[2291]: E0509 08:12:47.181807    2291 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="frontend:latest"
May 09 08:12:47 minikube kubelet[2291]: E0509 08:12:47.181904    2291 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="frontend:latest"
May 09 08:12:47 minikube kubelet[2291]: E0509 08:12:47.182114    2291 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:frontend,Image:frontend:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rdlmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod frontend-cf954c9f7-wdrch_default(c80782cc-b26e-4324-8228-1a0a179cc5a8): ErrImagePull: Error response from daemon: pull access denied for frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 09 08:12:47 minikube kubelet[2291]: E0509 08:12:47.183652    2291 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend\" with ErrImagePull: \"Error response from daemon: pull access denied for frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/frontend-cf954c9f7-wdrch" podUID="c80782cc-b26e-4324-8228-1a0a179cc5a8"
May 09 08:12:47 minikube kubelet[2291]: E0509 08:12:47.637905    2291 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend\" with ImagePullBackOff: \"Back-off pulling image \\\"backend:latest\\\": ErrImagePull: Error response from daemon: pull access denied for backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/backend-fb67664f-dtwpj" podUID="34a4327a-9c1d-48a8-b573-300ba65fd4fa"
May 09 08:12:52 minikube kubelet[2291]: E0509 08:12:52.285002    2291 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="frontend:latest"
May 09 08:12:52 minikube kubelet[2291]: E0509 08:12:52.285048    2291 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="frontend:latest"
May 09 08:12:52 minikube kubelet[2291]: E0509 08:12:52.285135    2291 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:frontend,Image:frontend:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bvbh5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod frontend-cf954c9f7-6dt5k_default(f938c8ca-8ed8-4431-9ee1-0906001d2e5e): ErrImagePull: Error response from daemon: pull access denied for frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 09 08:12:52 minikube kubelet[2291]: E0509 08:12:52.286298    2291 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend\" with ErrImagePull: \"Error response from daemon: pull access denied for frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/frontend-cf954c9f7-6dt5k" podUID="f938c8ca-8ed8-4431-9ee1-0906001d2e5e"
May 09 08:12:58 minikube kubelet[2291]: E0509 08:12:58.637219    2291 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend\" with ImagePullBackOff: \"Back-off pulling image \\\"backend:latest\\\": ErrImagePull: Error response from daemon: pull access denied for backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/backend-fb67664f-2xbng" podUID="db74514b-856a-4170-8331-48cbdc6f92c2"
May 09 08:13:02 minikube kubelet[2291]: E0509 08:13:02.637166    2291 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend\" with ImagePullBackOff: \"Back-off pulling image \\\"backend:latest\\\": ErrImagePull: Error response from daemon: pull access denied for backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/backend-fb67664f-dtwpj" podUID="34a4327a-9c1d-48a8-b573-300ba65fd4fa"
May 09 08:13:02 minikube kubelet[2291]: E0509 08:13:02.637222    2291 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend\" with ImagePullBackOff: \"Back-off pulling image \\\"frontend:latest\\\": ErrImagePull: Error response from daemon: pull access denied for frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/frontend-cf954c9f7-wdrch" podUID="c80782cc-b26e-4324-8228-1a0a179cc5a8"
May 09 08:13:05 minikube kubelet[2291]: E0509 08:13:05.637798    2291 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend\" with ImagePullBackOff: \"Back-off pulling image \\\"frontend:latest\\\": ErrImagePull: Error response from daemon: pull access denied for frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/frontend-cf954c9f7-6dt5k" podUID="f938c8ca-8ed8-4431-9ee1-0906001d2e5e"
May 09 08:13:09 minikube kubelet[2291]: E0509 08:13:09.637206    2291 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend\" with ImagePullBackOff: \"Back-off pulling image \\\"backend:latest\\\": ErrImagePull: Error response from daemon: pull access denied for backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/backend-fb67664f-2xbng" podUID="db74514b-856a-4170-8331-48cbdc6f92c2"
May 09 08:13:16 minikube kubelet[2291]: E0509 08:13:16.636639    2291 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend\" with ImagePullBackOff: \"Back-off pulling image \\\"frontend:latest\\\": ErrImagePull: Error response from daemon: pull access denied for frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/frontend-cf954c9f7-wdrch" podUID="c80782cc-b26e-4324-8228-1a0a179cc5a8"
May 09 08:13:17 minikube kubelet[2291]: E0509 08:13:17.639360    2291 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend\" with ImagePullBackOff: \"Back-off pulling image \\\"frontend:latest\\\": ErrImagePull: Error response from daemon: pull access denied for frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/frontend-cf954c9f7-6dt5k" podUID="f938c8ca-8ed8-4431-9ee1-0906001d2e5e"
May 09 08:13:21 minikube kubelet[2291]: E0509 08:13:21.637896    2291 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend\" with ImagePullBackOff: \"Back-off pulling image \\\"backend:latest\\\": ErrImagePull: Error response from daemon: pull access denied for backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/backend-fb67664f-2xbng" podUID="db74514b-856a-4170-8331-48cbdc6f92c2"
May 09 08:13:22 minikube kubelet[2291]: E0509 08:13:22.087082    2291 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="backend:latest"
May 09 08:13:22 minikube kubelet[2291]: E0509 08:13:22.087173    2291 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="backend:latest"
May 09 08:13:22 minikube kubelet[2291]: E0509 08:13:22.087358    2291 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:backend,Image:backend:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:NODE_ENV,Value:production,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sn52z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod backend-fb67664f-dtwpj_default(34a4327a-9c1d-48a8-b573-300ba65fd4fa): ErrImagePull: Error response from daemon: pull access denied for backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 09 08:13:22 minikube kubelet[2291]: E0509 08:13:22.088668    2291 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend\" with ErrImagePull: \"Error response from daemon: pull access denied for backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/backend-fb67664f-dtwpj" podUID="34a4327a-9c1d-48a8-b573-300ba65fd4fa"
May 09 08:13:35 minikube kubelet[2291]: E0509 08:13:35.636998    2291 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend\" with ImagePullBackOff: \"Back-off pulling image \\\"backend:latest\\\": ErrImagePull: Error response from daemon: pull access denied for backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/backend-fb67664f-dtwpj" podUID="34a4327a-9c1d-48a8-b573-300ba65fd4fa"
May 09 08:13:46 minikube kubelet[2291]: E0509 08:13:46.637399    2291 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend\" with ImagePullBackOff: \"Back-off pulling image \\\"backend:latest\\\": ErrImagePull: Error response from daemon: pull access denied for backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/backend-fb67664f-dtwpj" podUID="34a4327a-9c1d-48a8-b573-300ba65fd4fa"
May 09 08:13:47 minikube kubelet[2291]: E0509 08:13:47.682681    2291 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving" image="frontend:latest"
May 09 08:13:47 minikube kubelet[2291]: E0509 08:13:47.682781    2291 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving" image="frontend:latest"
May 09 08:13:47 minikube kubelet[2291]: E0509 08:13:47.683204    2291 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:frontend,Image:frontend:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rdlmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod frontend-cf954c9f7-wdrch_default(c80782cc-b26e-4324-8228-1a0a179cc5a8): ErrImagePull: Error response from daemon: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving" logger="UnhandledError"
May 09 08:13:47 minikube kubelet[2291]: E0509 08:13:47.684605    2291 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend\" with ErrImagePull: \"Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/frontend-cf954c9f7-wdrch" podUID="c80782cc-b26e-4324-8228-1a0a179cc5a8"


==> storage-provisioner [62aff1029356] <==
I0509 08:05:56.551947       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0509 08:05:56.582949       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0509 08:05:56.583646       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0509 08:05:56.607448       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0509 08:05:56.607813       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_1b275036-f787-4773-b419-cc32e1ec16fe!
I0509 08:05:56.607754       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"e223ee7e-d76e-4f5a-abe4-d1329f884038", APIVersion:"v1", ResourceVersion:"424", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_1b275036-f787-4773-b419-cc32e1ec16fe became leader
I0509 08:05:56.709134       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_1b275036-f787-4773-b419-cc32e1ec16fe!


==> storage-provisioner [e49c2ca162c1] <==
I0509 08:05:25.202030       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0509 08:05:55.242523       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

